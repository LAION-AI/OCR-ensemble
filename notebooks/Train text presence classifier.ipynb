{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e90818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ocr_ensemble.data import load_dataset_1K, load_dataset_10K\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "import json\n",
    "from functools import partial\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "with open('../data/laion2b-en-10K-labels.json', 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "def key2label(key, labels):\n",
    "    return labels[key]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "\n",
    "clip_preproc = T.Lambda(lambda img: preprocess(transform(rearrange(torch.tensor(img), 'a b c -> c a b'))))\n",
    "\n",
    "dataset = load_dataset_10K()\n",
    "dataset2 = dataset.map_tuple(clip_preproc, identity, partial(key2label, labels=labels))\n",
    "loader = torch.utils.data.DataLoader(dataset2, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:14,  2.76it/s]"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "caption_features = []\n",
    "targets = []\n",
    "for imgs, captions, labels in tqdm(loader):\n",
    "    feats = model.encode_image(imgs.to(device))\n",
    "    caption_feats = model.encode_text(clip.tokenize([c[:77] for c in captions]).to(device))\n",
    "    features += [feats.cpu().detach()]\n",
    "    caption_features += [caption_feats.cpu().detach()]\n",
    "    targets += [labels.detach()]\n",
    "features = torch.cat(features, axis=0)\n",
    "targets = torch.cat(targets, axis=0)\n",
    "caption_features = torch.cat(caption_features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a323d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_normalized = F.normalize(features.float(), dim=1)\n",
    "caption_features_normalized = F.normalize(caption_features.float(), dim=1)\n",
    "combined = torch.cat([features_normalized, caption_features_normalized], axis=1)\n",
    "cv = LogisticRegressionCV(Cs=[0.1, 1, 5, 10, 100])\n",
    "cv.fit(features_normalized, targets)\n",
    "print(cv.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e409dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = F.normalize(caption_features[targets.bool()].mean(axis=0, keepdim=True).float(), dim=1)\n",
    "text_feature = model.encode_text(clip.tokenize('contains handwritten or printed text').to(device)).cpu().detach()\n",
    "text_feature = F.normalize(text_feature.float(), dim=1)\n",
    "notext_feature = F.normalize(caption_features[~targets.bool()].mean(axis=0, keepdim=True).float(), dim=1)\n",
    "label_embs = torch.cat([notext_feature, text_feature], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(F.softmax((features_normalized.float() @ label_embs.T), dim=1)[:, 1] > 0.495).int().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c394bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "((F.softmax((features_normalized.float() @ label_embs.T), dim=1)[:, 1] > 0.495).int() == targets).float().sum() / len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(targets, (F.softmax((features_normalized.float() @ label_embs.T), dim=1)[:, 1] > 0.495).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33cadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((features_normalized.float() @ label_embs.T).argmax(axis=1).sum())\n",
    "print(((features_normalized.float() @ label_embs.T).argmax(axis=1) == targets).float().sum() / len(targets))\n",
    "print(confusion_matrix(targets, (features_normalized.float() @ label_embs.T).argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09aefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions normalized: tensor(0.6759)\n",
    "# captions unnormalized: tensor(0.4354)\n",
    "# presence text, absense caption mean normalized: tensor(0.7191)\n",
    "# img feats normalized: tensor(0.8250) \n",
    "# img feats unnormalized: tensor(0.5728)\n",
    "# presence text, absense img feats mean normalized: tensor(0.5651)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b996fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8*len(features))\n",
    "X_train = features[:n_train]\n",
    "y_train = targets[:n_train]\n",
    "X_test = features[n_train:]\n",
    "y_test = targets[n_train:]\n",
    "clf = LogisticRegression(C=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=10)\n",
    "clf.fit(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e3303",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../models', exist_ok = True)\n",
    "with open('../models/presence_clf.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db043b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/presence_clf.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef7d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
